{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone as pcn\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown\n",
    "import gradio as gr\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 1106}},\n",
       " 'total_vector_count': 1106}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure client\n",
    "pc = pcn(api_key=api_key)\n",
    "index_name = 'recipes-index'\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Set Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-3-small\"\n",
    "embed = OpenAIEmbeddings(model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = \"description\"\n",
    "\n",
    "vectorStore = PineconeVectorStore(\n",
    "  index, embed, text_field\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0.0  \n",
    ")\n",
    "\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "# we can set a similarity score threshold and only return documents with a score above that threshold.\n",
    "# search_kwargs={\"score_threshold\": 0.5}\n",
    "# We can also limit the number of documents k returned by the retriever.\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe(query, history=None):# completion llm  \n",
    "\n",
    "\n",
    "    # 2. Incorporate the retriever into a question-answering chain.\n",
    "    system_prompt = (\n",
    "    \"\"\"\n",
    "        Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "        Take the information from context[0][\"metadata\"]\n",
    "        Then follow this format\n",
    "        Title: [Title of the Recipe]\n",
    "        Description: [Short Description of the Recipe]\n",
    "        Preparation Time: [Preparation Time]\n",
    "        Cooking Time: [Cooking Time]\n",
    "        Difficulty: [Difficulty Level]\n",
    "        Serves: [Number of Servings]\n",
    "        Diet Type: [Diet Type]\n",
    "        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "        Ingredients: [List of Ingredients]\n",
    "        Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "        If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "        {context}\n",
    "        {{metadata}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": query})\n",
    "    print(response)\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7884\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7888\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### ragchain bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7fc5bcf7e900>, search_kwargs={'k': 3}), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\n        Always add a kind and friendly message according to the context at the beginning of the response.\\n        Take the information from context[0][\"metadata\"]\\n        Then follow this format:\\n        Title: [Title of the Recipe]\\n        Description: [Short Description of the Recipe]\\n        Preparation Time: [Preparation Time]\\n        Cooking Time: [Cooking Time]\\n        Difficulty: [Difficulty Level]\\n        Serves: [Number of Servings]\\n        Diet Type: [Diet Type]\\n        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\\n        Ingredients: [List of Ingredients]\\n        Instructions: [Step-by-step Cooking Instructions]\\n\\n        If you don\\'t have any general information, just respond with \"I don\\'t know!\"\\n\\n        {context}\\n        {{metadata}}\\n        '))])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc5bc08af90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc5bc1d0890>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n",
      "#### messages [('human', 'What can I do with brocolli')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1518, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 793, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 623, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_6021/3261222074.py\", line 62, in get_recipe\n",
      "    response = rag_chain.invoke({\"input\": messages})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2877, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py\", line 495, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1786, in _call_with_config\n",
      "    context.run(\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 398, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py\", line 482, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3580, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3564, in _invoke_step\n",
      "    return context.run(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 252, in invoke\n",
      "    raise e\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 245, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 1042, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_pinecone/vectorstores.py\", line 379, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_pinecone/vectorstores.py\", line 322, in similarity_search_with_score\n",
      "    self._embedding.embed_query(query), k=k, filter=filter, namespace=namespace\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py\", line 598, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py\", line 558, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py\", line 453, in _get_len_safe_embeddings\n",
      "    _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py\", line 414, in _tokenize\n",
      "    token = encoding.encode_ordinary(text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/tiktoken/core.py\", line 69, in encode_ordinary\n",
      "    return self._core_bpe.encode_ordinary(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument 'text': 'list' object cannot be converted to 'PyString'\n"
     ]
    }
   ],
   "source": [
    "def get_recipe(query, history=True):\n",
    "    \"\"\"\n",
    "    The query can be a string like \"chicken recipe\". This function will\n",
    "    return the recipe that matches the query the most.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "        history (list): A list of dictionaries containing previous interactions.\n",
    "\n",
    "    Returns:\n",
    "        str: The recipe that matches the query the most.\n",
    "    \"\"\"\n",
    "    # 1. Create a prompt\n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "        Important instruction:\n",
    "        When the user says phrases like \"give me a recipe\", \"give me something to cook\", or any variation of \"give me\" followed by a recipe request, interpret this as a request to *recommend* a recipe.\n",
    "        Take the information from context[0][\"metadata\"]\n",
    "        Then follow this format:\n",
    "        Title: [Title of the Recipe]\n",
    "        Description: [Short Description of the Recipe]\n",
    "        Preparation Time: [Preparation Time]\n",
    "        Cooking Time: [Cooking Time]\n",
    "        Difficulty: [Difficulty Level]\n",
    "        Serves: [Number of Servings]\n",
    "        Diet Type: [Diet Type]\n",
    "        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "        Ingredients: [List of Ingredients]\n",
    "        Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "        If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "        {context}\n",
    "        {{metadata}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # 2. Create a chain\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Add previous history to the prompt if available\n",
    "    messages = []\n",
    "    \n",
    "    if history:\n",
    "        for item in history:\n",
    "            role = item[\"role\"]\n",
    "            content = item[\"content\"]\n",
    "            messages.append((role, content))\n",
    "    \n",
    "    # Add the current query\n",
    "    messages.append((\"human\", query))\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    print(\"#### ragchain\", rag_chain)\n",
    "    print(\"#### messages\", messages)\n",
    "\n",
    "    # 4. Run the chain with history\n",
    "    response = rag_chain.invoke({\"input\": messages})\n",
    "    print(\"### resopnse \",response)\n",
    "    \n",
    "    # 5. Return the response\n",
    "    return response[\"answer\"]\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\", \"please recommend 3 recipes with rice\"],\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Inicializando el modelo y el retriever\n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0.0  \n",
    ")\n",
    "\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# Prompt para contextualizar la pregunta\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "def get_recipe(query, history=None):\n",
    "    # Inicializa el historial de chat si es la primera vez\n",
    "    history = []    # Prompt del sistema que guía cómo generar la respuesta\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "        Take the information from context[0][\"metadata\"]\n",
    "        Then follow this format\n",
    "        Title: [Title of the Recipe]\n",
    "        Description: [Short Description of the Recipe]\n",
    "        Preparation Time: [Preparation Time]\n",
    "        Cooking Time: [Cooking Time]\n",
    "        Difficulty: [Difficulty Level]\n",
    "        Serves: [Number of Servings]\n",
    "        Diet Type: [Diet Type]\n",
    "        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "        Ingredients: [List of Ingredients]\n",
    "        Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "        If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "        {context}\n",
    "        {{metadata}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Configura el prompt del QA (sistema y humano)\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Crear las cadenas para el QA y el RAG (retrieval-augmented generation)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    # Invoca la cadena con la pregunta y el historial de chat\n",
    "    response = rag_chain.invoke({\"input\": query, \"chat_history\": history})\n",
    "\n",
    "    # Actualiza el historial de chat con los nuevos mensajes\n",
    "    history.extend(\n",
    "        [\n",
    "            HumanMessage(content=query),\n",
    "            AIMessage(content=response[\"answer\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Imprimir el mensaje generado por la IA\n",
    "    print(\"####### ai_msg_1:\", response)\n",
    "\n",
    "    # Devolver el contenido del mensaje de la IA\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7885\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\", \"please recommend 3 recipes with rice\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recipes",
   "language": "python",
   "name": "venv_recipes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
