{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acrisvall/recipes_rag/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone as pcn\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown\n",
    "import gradio as gr\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 1106}},\n",
       " 'total_vector_count': 1106}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure client\n",
    "pc = pcn(api_key=api_key)\n",
    "index_name = 'recipes-index'\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Set Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-3-small\"\n",
    "embed = OpenAIEmbeddings(model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = \"description\"\n",
    "\n",
    "vectorStore = PineconeVectorStore(\n",
    "  index, embed, text_field\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0.0  \n",
    ")\n",
    "\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "# we can set a similarity score threshold and only return documents with a score above that threshold.\n",
    "# search_kwargs={\"score_threshold\": 0.5}\n",
    "# We can also limit the number of documents k returned by the retriever.\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe(query, history=None):# completion llm  \n",
    "\n",
    "\n",
    "    # 2. Incorporate the retriever into a question-answering chain.\n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "        Important instruction:\n",
    "        When the user says phrases like \"give me a recipe\", \"give me something to cook\", or any variation of \"give me\" followed by a recipe request, interpret this as a request to *recommend* a recipe.\n",
    "        Take the information from context[0][\"metadata\"]\n",
    "        Then follow this format:\n",
    "        Title: [Title of the Recipe]\n",
    "        Description: [Short Description of the Recipe]\n",
    "        Preparation Time: [Preparation Time]\n",
    "        Cooking Time: [Cooking Time]\n",
    "        Difficulty: [Difficulty Level]\n",
    "        Serves: [Number of Servings]\n",
    "        Diet Type: [Diet Type]\n",
    "        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "        Ingredients: [List of Ingredients]\n",
    "        Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "        If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "        {context}\n",
    "        {{metadata}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": query})\n",
    "    print(response)\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "# Prompt para contextualizar la pregunta\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = (\n",
    "      \"\"\"\n",
    "      Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "      Take the information from context[0][\"metadata\"]\n",
    "      Then follow this format\n",
    "      Title: [Title of the Recipe]\n",
    "      Description: [Short Description of the Recipe]\n",
    "      Preparation Time: [Preparation Time]\n",
    "      Cooking Time: [Cooking Time]\n",
    "      Difficulty: [Difficulty Level]\n",
    "      Serves: [Number of Servings]\n",
    "      Diet Type: [Diet Type]\n",
    "      Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "      Ingredients: [List of Ingredients]\n",
    "      Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "      If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "      {context}\n",
    "      {{metadata}}\n",
    "      \"\"\"\n",
    "  )\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "      (\"system\", system_prompt),\n",
    "      MessagesPlaceholder(\"chat_history\"),\n",
    "      (\"human\", \"{input}\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful Management of chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Our graph consists only of one node:\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "def get_recipe(query, history=None):\n",
    "\n",
    "  result = app.invoke(\n",
    "    {\"input\": query},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "  print(\"#### result\", result[\"answer\"])\n",
    "  return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\", \"please recommend 3 recipes with rice\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What can I do with brocolli\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Broccoli is such a versatile vegetable, and there are many delicious ways to use it. Here are a few ideas:\n",
      "\n",
      "1. **Broccoli Soup**: Blend cooked broccoli with vegetable broth, cream, and seasonings for a creamy soup. Top with a cheesy crumble for added flavor.\n",
      "\n",
      "2. **Roasted Broccoli**: Toss broccoli florets with olive oil, salt, and pepper, then roast in the oven until crispy. This makes a great side dish!\n",
      "\n",
      "3. **Stir-Fry**: Add broccoli to your favorite stir-fry with other vegetables and protein for a quick and healthy meal.\n",
      "\n",
      "4. **Salads**: Use raw or blanched broccoli in salads. Pair it with a vinaigrette made from capers, mustard, and honey for a tasty side.\n",
      "\n",
      "5. **Falafel**: Use broccoli stalks to make falafel by blending them with chickpeas and spices, reducing food waste while creating a delicious dish.\n",
      "\n",
      "Feel free to ask if you want specific recipes or more ideas!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "please specify the broccoli soup\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Of course! Here’s a delightful recipe for Broccoli Soup that you can enjoy. \n",
      "\n",
      "Title: Cheesy Broccoli Soup  \n",
      "Description: A hearty and creamy broccoli soup, perfect for a starter or a cozy lunch, enhanced with a cheesy, seedy crumble.  \n",
      "Preparation Time: 15 minutes  \n",
      "Cooking Time: 4 hours (slow cooker)  \n",
      "Difficulty: Easy  \n",
      "Serves: 4  \n",
      "Diet Type: Vegetarian  \n",
      "Nutrition: calories 250 | fat 10g | protein 8g | fiber 5g  \n",
      "\n",
      "Ingredients:  \n",
      "- 4 cups frozen broccoli florets  \n",
      "- 1 cup frozen cauliflower florets  \n",
      "- 1 cup brown rice  \n",
      "- 4 cups vegetable broth  \n",
      "- 1 cup cream (or a dairy-free alternative)  \n",
      "- 1 cup shredded cheese (cheddar or your choice)  \n",
      "- 1/2 cup breadcrumbs  \n",
      "- 2 tablespoons seeds (like pumpkin or sunflower)  \n",
      "- Salt and pepper to taste  \n",
      "- Optional: Crusty bread for serving  \n",
      "\n",
      "Instructions:  \n",
      "1. In a slow cooker, combine the frozen broccoli, cauliflower, brown rice, and vegetable broth.  \n",
      "2. Season with salt and pepper, then cover and cook on low for about 4 hours, or until the rice is tender.  \n",
      "3. Once cooked, stir in the cream and shredded cheese until melted and well combined.  \n",
      "4. In a small bowl, mix the breadcrumbs with the seeds and a little olive oil. Spread this mixture on a baking sheet and toast in the oven until golden brown.  \n",
      "5. Serve the soup hot, topped with the cheesy, seedy crumble and enjoy with crusty bread on the side.  \n",
      "\n",
      "Enjoy your delicious and comforting broccoli soup!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "the broccoli soup what is good for?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Broccoli soup is not only delicious but also packed with health benefits. Here are some reasons why it's good for you:\n",
      "\n",
      "1. **Nutrient-Rich**: Broccoli is high in vitamins C, K, and A, as well as folate and fiber, which are essential for overall health.\n",
      "\n",
      "2. **Antioxidant Properties**: The antioxidants in broccoli can help reduce inflammation and protect your cells from damage.\n",
      "\n",
      "3. **Digestive Health**: The fiber content in broccoli supports healthy digestion and can help prevent constipation.\n",
      "\n",
      "4. **Weight Management**: Broccoli soup can be low in calories while being filling, making it a great option for those looking to manage their weight.\n",
      "\n",
      "5. **Heart Health**: The nutrients in broccoli can contribute to heart health by helping to lower cholesterol levels and improve blood vessel function.\n",
      "\n",
      "6. **Bone Health**: With its high vitamin K content, broccoli supports bone health and may help prevent osteoporosis.\n",
      "\n",
      "7. **Immune Support**: The vitamins and minerals in broccoli can boost your immune system, helping your body fight off infections.\n",
      "\n",
      "Enjoying broccoli soup can be a tasty way to incorporate these health benefits into your diet! If you have any more questions or need further information, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "def get_recipe(query, history=None):\n",
    "    # Inicializa el historial de chat si es la primera vez\n",
    "    history = []    # Prompt del sistema que guía cómo generar la respuesta\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        Always add a kind and friendly message according to the context at the beginning of the response.\n",
    "        Take the information from context[0][\"metadata\"]\n",
    "        Then follow this format\n",
    "        Title: [Title of the Recipe]\n",
    "        Description: [Short Description of the Recipe]\n",
    "        Preparation Time: [Preparation Time]\n",
    "        Cooking Time: [Cooking Time]\n",
    "        Difficulty: [Difficulty Level]\n",
    "        Serves: [Number of Servings]\n",
    "        Diet Type: [Diet Type]\n",
    "        Nutrition: calories context[0][\"metadata\"][\"calories\"] | fat context[0][\"metadata\"][\"fat\"] | protein context[0][\"metadata\"][\"protein\"] | fiber context[0][\"metadata\"][\"fibre\"]\n",
    "        Ingredients: [List of Ingredients]\n",
    "        Instructions: [Step-by-step Cooking Instructions]\n",
    "\n",
    "        If you don't have any general information, just respond with \"I don't know!\"\n",
    "\n",
    "        {context}\n",
    "        {{metadata}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Configura el prompt del QA (sistema y humano)\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Crear las cadenas para el QA y el RAG (retrieval-augmented generation)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    # Invoca la cadena con la pregunta y el historial de chat\n",
    "    response = rag_chain.invoke({\"input\": query, \"chat_history\": history})\n",
    "\n",
    "    # Actualiza el historial de chat con los nuevos mensajes\n",
    "    history.extend(\n",
    "        [\n",
    "            HumanMessage(content=query),\n",
    "            AIMessage(content=response[\"answer\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Imprimir el mensaje generado por la IA\n",
    "    print(\"####### ai_msg_1:\", response)\n",
    "\n",
    "    # Devolver el contenido del mensaje de la IA\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(\n",
    "  get_recipe,\n",
    "  title=\"Recipes Generator\",\n",
    "  description=\"Ask recipes recommendations based on ingredients and instructions\",\n",
    "  examples=[\"Recommend a recipe with chicken\", \"A low-calorie chicken recipe\", \"What can I do with brocolli\", \"please recommend 3 recipes with rice\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recipes",
   "language": "python",
   "name": "venv_recipes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
